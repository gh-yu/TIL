## 특정 검색어 뉴스의 제목과 링크 크롤링

```python
import requests
from bs4 import BeautifulSoup 

# url에서 한글은 인코딩되어서 표시됨
response = requests.get("https://search.naver.com/search.naver?where=news&sm=tab_jum&query=%EC%82%BC%EC%84%B1%EC%A0%84%EC%9E%90")
html = response.text # html 소스코드
soup = BeautifulSoup(html, 'html.parser')
# CSS Selector로 태그 요소 가져오기
links = soup.select('.news_tit') # 결과는 리스트

for link in links:
    title = link.text # 태그 안에 텍스트요소를 가져옴
    url = link.attrs['href'] # href의 속성값을 가져옴
    print(title, url)
```

**실행 결과**
![실행 결과](https://user-images.githubusercontent.com/78662802/155575933-ed77cb10-5b0f-4075-8805-c46ee8951249.png)


<br>
<hr>

## 검색어를 입력 받아 해당 검색어로 크롤링 : pyautogui

`프로그램을 실행하면 검색어를 입력 받게 해서 해당 검색어로 크롤링 되게 만들기`

### URL


- 인터넷 주소 형식

    ![url](https://user-images.githubusercontent.com/78662802/155575928-238409b5-41c6-4274-818b-20950b7638ad.png)
    
- Protocol - Domain - Path - Parameter
    - Protocol : http 또는 https
    - Domain : IP 주소에 이름을 부여한 것
    - Path : 서버에서 해당 페이지의 경로
    - Parameter : key & value 형태, 서버에 추가적인 정보 제공하기 위한 것
        - 네이버 뉴스 검색시 url의 query ← 검색어 parameter에 해당함
    
    ![url 구조](https://user-images.githubusercontent.com/78662802/155575924-41b885ae-38f9-44d7-9446-05d65513c655.png)
    

### 사용자입력

`사용자가 입력한 값을 어떤 변수에 대입하기`


1. **input**의 사용
    
    input은 입력되는 모든 것을 문자열로 취급
    
    > `a = input()`
    > 
    
    > `input(”질문 내용”)`
    > 
2. **pyautogui** 라이브러리로 간단한 팝업창 띄워서 사용자 입력 받기
    
    **pyautogui**
    
    - 마우스, 키보드 매크로 라이브러리
    - 간단한 입력 창 띄우기
    - 설치 : `pip install pyautogui`
    
    **pyautogui 입력 창**
    
    - `pyautogui.prompt(”검색어를 입력하세요”)`
        
        ![pyautogui 입력 창](https://user-images.githubusercontent.com/78662802/155575917-2e4b056a-747f-43b4-b286-57f27fa0a6e7.png)
        

### 최종 코드


```python
import requests
from bs4 import BeautifulSoup
import pyautogui

# keyword = input("검색어를 입력하세요>>>") # input 내장함수 사용하여 입력 값 받기
keyword = pyautogui.prompt("검색어를 입력하세요") # pyautogui 라이브러리 사용하여 입력 창 띄우기
response = requests.get("https://search.naver.com/search.naver?where=news&sm=tab_jum&query=" + keyword)
html = response.text
soup = BeautifulSoup(html, 'html.parser')
links = soup.select('.news_tit')

for link in links:
    title = link.text
    url = link.attrs['href']
    print(title, url)
```

<br>
<hr>

## 여러 페이지 결과 크롤링 : 반복문



### **변경되는 url 분석하기**

- https://search.naver.com/search.naver?where=news&sm=tab_pge&query=삼성전자&sort=0&photo=0&field=0&pd=0&ds=&de=&cluster_rank=87&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so:r,p:all,a:all&start=41
    - 1페이지 : start=1
    - 2페이지 : start=11
    - 3페이지 : start=21
    - ..
    - 5페이지 : start=41
    - start가 10씩 증가

### 반복문 사용

- for i in range(시작, 끝, 단계):

### 최종 코드


```python
import requests
from bs4 import BeautifulSoup
import pyautogui

keyword = pyautogui.prompt("검색어를 입력하세요")
lastPage = int(pyautogui.prompt("마지막 페이지번호를 입력해 주세요")) # 사용자로부터 받은 데이터 = 문자열 -> int로 형변환
pageNum = 1;
maxStart = lastPage * 10;
for i in range(1, maxStart, 10):
    print(f"{pageNum}페이지 입니다. =============================")
    response = requests.get("https://search.naver.com/search.naver?where=news&sm=tab_jum&query=" + keyword + "start=" + str(i))
    html = response.text
    soup = BeautifulSoup(html, 'html.parser')
    links = soup.select('.news_tit')

    for link in links:
        title = link.text
        url = link.attrs['href']
        print(title, url)
    
    pageNum = pageNum + 1
```
